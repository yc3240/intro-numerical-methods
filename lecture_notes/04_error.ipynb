{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources of Error\n",
    "\n",
    "Error can come from many sources when using applying a numerical method:\n",
    " - Model/Data Error\n",
    " - Truncation Error\n",
    " - Floating Point Error\n",
    " \n",
    "**Goal:** Categorize and understand each type of error and explore some simple approaches to analyzing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model and Data Error\n",
    "\n",
    "Errors in fundamental formulation\n",
    " - Lotka-Volterra - fractional rabbits, no extinctions, etc.\n",
    " - Data Error - Inaccuracy in measurement or uncertainties in parameters\n",
    " \n",
    "Unfortunatley we cannot control model and data error directly but we can use methods that may be more robust in the presense of these types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Truncation Error\n",
    "\n",
    "Errors arising from approximating a function with a simpler function (e.g. $sin(x) \\approx x$ for $|x| \\approx 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Floating Point Error\n",
    "\n",
    "Errors arising from approximating real numbers with finite-precision numbers and arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Definitions\n",
    "\n",
    "Given a true value of a function $f$ and an approximate solution $\\hat{f}$ define:\n",
    "\n",
    "Absolute Error:  $e = |f - \\hat{f}|$\n",
    "\n",
    "Relative Error:  $r = \\frac{e}{|f|} = \\frac{|f - \\hat{f}|}{|f|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decimal precision $p$ is defined as the minimum value that satisfies\n",
    "\n",
    "$$x = \\text{round}(10^{-n} \\cdot x) \\cdot 10^n$$\n",
    "\n",
    "where\n",
    "\n",
    "$$n = \\text{floor}(\\log_{10} x) + 1 - p$$\n",
    "\n",
    "Note that if we are asking the decimal precision of the approximation $\\hat{f}$ of $f$ then we need to use the absolute error to determine the precision.  To find the decimal precision in this case look at the magnitude of the absolute error and deterimine the place of the first error.  Combine this with the number of \"correct\" digits and you will get the decimal precision of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Truncation Error and Taylor's Theorem\n",
    "\n",
    "**Taylor's Theorem:**  Let $f(x) \\in C^{m+1}[a,b]$ and $x_0 \\in [a,b]$, then for all $x \\in (a,b)$ there exists a number $c = c(x)$ that lies between $x_0$ and $x$ such that\n",
    "\n",
    "$$ f(x) = T_N(x) + R_N(x)$$\n",
    "\n",
    "where $T_N(x)$ is the Taylor polynomial approximation\n",
    "\n",
    "$$T_N(x) = \\sum^N_{n=0} \\frac{f^{(n)}(x_0)\\cdot(x-x_0)^n}{n!}$$\n",
    "\n",
    "and $R_N(x)$ is the residual (the part of the series we left off)\n",
    "\n",
    "$$R_N(x) = \\frac{f^{(n+1)}(c) \\cdot (x - x_0)^{n+1}}{(n+1)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another way to think about these results involves replacing $x - x_0$ with $\\Delta x$.  The primary idea here is that the residual $R_N(x)$ becomes smaller as $\\Delta x \\rightarrow 0$.\n",
    "\n",
    "$$T_N(x) = \\sum^N_{n=0} \\frac{f^{(n)}(x_0)\\cdot\\Delta x^n}{n!}$$\n",
    "\n",
    "and $R_N(x)$ is the residual (the part of the series we left off)\n",
    "\n",
    "$$R_N(x) = \\frac{f^{(n+1)}(c) \\cdot \\Delta x^{n+1}}{(n+1)!} \\leq M \\Delta x^{n+1} = O(\\Delta x^{n+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example 1\n",
    "\n",
    "$f(x) = e^x$ with $x_0 = 0$\n",
    "\n",
    "Using this we can find expressions for the relative and absolute error as a function of $x$ assuming $N=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f'(x) = e^x, ~~~ f''(x) = e^x ~~~ f^{(n)}(x) = e^x$$\n",
    "\n",
    "$$T_N(x) = \\sum^N_{n=0} e^0 \\frac{x^n}{n!} ~~~~\\Rightarrow ~~~~ T_2(x) = 1 + x + \\frac{x^2}{2}$$\n",
    "\n",
    "$$R_N(x) = e^c \\frac{x^{n+1}}{(n+1)!} = e^c \\cdot \\frac{x^3}{6} ~~~~ \\Rightarrow ~~~~ R_2(x) \\leq \\frac{e^1}{6} \\approx 0.5$$\n",
    "\n",
    "$$e^1 = 2.718\\ldots$$\n",
    "\n",
    "$$T_2(1) = 2.5 \\Rightarrow e \\approx 0.2 ~~ r \\approx 0.1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also use the package sympy which has the ability to calculate Taylor polynomials built-in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "x = sympy.symbols('x')\n",
    "f = sympy.symbols('f', cls=sympy.Function)\n",
    "\n",
    "f = sympy.exp(x)\n",
    "f.series(x0=0, n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Lets plot this numerically for a section of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(-1, 1, 100)\n",
    "T_N = 1.0 + x + x**2 / 2.0\n",
    "R_N = numpy.exp(1) * x**3 / 6.0\n",
    "\n",
    "plt.plot(x, T_N, 'r', x, numpy.exp(x), 'k', x, R_N, 'b')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$f(x)$, $T_N(x)$, $R_N(x)$\")\n",
    "plt.legend([\"$T_N(x)$\", \"$f(x)$\", \"$R_N(x)$\"], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example 2\n",
    "\n",
    "$f(x) = \\frac{1}{x} ~~~~~~ x_0  = 1$, approximate with $\\hat{f}(x) = T_2(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f'(x) = -\\frac{1}{x^2} ~~~~~~~ f''(x) = \\frac{2}{x^3} ~~~~~~~ f^{(n)}(x) = \\frac{(-1)^n n!}{x^{n+1}}$$\n",
    "\n",
    "$$T_N(x) = \\sum^N_{n=0} (-1)^n (x-1)^n ~~~~ \\Rightarrow ~~~~ T_2(x) = 1 - (x - 1) + (x - 1)^2$$\n",
    "\n",
    "$$R_N(x) = \\frac{(-1)^{n+1}(x - 1)^{n+1}}{c^{n+2}} ~~~~ \\Rightarrow ~~~~ R_2(x) = \\frac{-(x - 1)^{3}}{c^{4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(0.8, 2, 100)\n",
    "T_N = 1.0 - (x-1) + (x-1)**2\n",
    "R_N = -(x-1.0)**3 / (1.1**4)\n",
    "\n",
    "plt.plot(x, T_N, 'r', x, 1.0 / x, 'k', x, R_N, 'b')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$f(x)$, $T_N(x)$, $R_N(x)$\")\n",
    "\n",
    "plt.legend([\"$T_N(x)$\", \"$f(x)$\", \"$R_N(x)$\"], loc=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Symbols and Definitions\n",
    "\n",
    "Big-O notation: $f(x) = \\text{O}(g(x))$ as $x \\rightarrow a$ if and only if $|f(x)| \\leq M |g(x)|$ as $|x - a| < \\delta$ for $M$ and $a$ positive.\n",
    "\n",
    "In practice we use Big-O notation to say something about how the terms we may have left out of a series might behave.  We saw an example earlier of this with the Taylor's series approximations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:\n",
    "$f(x) = \\sin x$ with $x_0 = 0$ then\n",
    "\n",
    "$$T_N(x) = \\sum^N_{n=0} (-1)^{n} \\frac{x^{2n+1}}{(2n+1)!}$$\n",
    "\n",
    "We can actually write $f(x)$ then as\n",
    "\n",
    "$$f(x) = x - \\frac{x^3}{6} + \\frac{x^5}{120} + O(x^7)$$\n",
    "\n",
    "This becomes more useful when we look at this as we did before with $\\Delta x$:\n",
    "\n",
    "$$f(x) = \\Delta x - \\frac{\\Delta x^3}{6} + \\frac{\\Delta x^5}{120} + O(\\Delta x^7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**We can also develop rules for error propagation based on Big-O notation:**\n",
    "\n",
    "In general, there are two theorems that do not need proof and hold when the value of x is large: \n",
    "\n",
    "Let\n",
    "$$f(x) = p(x) + O(x^n)$$\n",
    "$$g(x) = q(x) + O(x^m)$$\n",
    "$$k = \\max(n, m)$$ then\n",
    "$$f+g = p + q + O(x^k)$$\n",
    "$$f \\cdot g = p \\cdot q + p \\mathcal{O}(x^m) + q \\mathcal{O}(x^n) + O(x^{n + m}) = p \\cdot q + \\mathcal{O}(x^{n+m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, if we are interested in small values of x, say Δx, the above expressions can be modified as follows: \n",
    "\n",
    "$$f(\\Delta x) = p(\\Delta x) + O(\\Delta x^n)$$\n",
    "$$g(\\Delta x) = q(\\Delta x) + O(\\Delta x^m)$$\n",
    "$$r = \\min(n, m)$$ then\n",
    "\n",
    "$$f+g = p + q + O(\\Delta x^r)$$\n",
    "\n",
    "$$f \\cdot g = p \\cdot q + p \\cdot O(\\Delta x^m) + q \\cdot O(\\Delta x^n) + O(\\Delta x^{n+m}) = p \\cdot q + O(\\Delta x^r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note 1:** In this case we suppose that at least the polynomial with $k = \\max(n, m)$ has the following form: \n",
    "$$p(\\Delta x) = 1 + p_1 \\Delta x + p_2 \\Delta x^2 + ...$$\n",
    "or $$q(\\Delta x) = 1 + q_1 \\Delta x + q_2 \\Delta x^2 + ...$$\n",
    "so that there is an O(1) term that guarantees the existence of $O(\\Delta x^r)$ in the final product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To get a sense of why we care most about the power on $\\Delta x$ when considering convergence the following figure shows how different powers on the convergence rate can effect how quickly we converge to our solution.  Note that here we are plotting the same data two different ways.  Plotting the error as a function of $\\Delta x$ is a common way to show that a numerical method is doing what we expect and exhibits the correct convergence behavior.  Since errors can get small quickly it is very common to plot these sorts of plots on a log-log scale to easily visualize the results.  Note that if a method was truly of the order $n$ that they will be a linear function in log-log space with slope $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dx = numpy.linspace(1.0, 1e-4, 100)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2.0)\n",
    "axes = []\n",
    "axes.append(fig.add_subplot(1, 2, 1))\n",
    "axes.append(fig.add_subplot(1, 2, 2))\n",
    "\n",
    "for n in xrange(1, 5):\n",
    "    axes[0].plot(dx, dx**n, label=\"$\\Delta x^%s$\" % n)\n",
    "    axes[1].loglog(dx, dx**n, label=\"$\\Delta x^%s$\" % n)\n",
    "\n",
    "axes[0].legend(loc=2)\n",
    "axes[1].set_xticks([10.0**(-n) for n in xrange(5)])\n",
    "axes[1].set_yticks([10.0**(-n) for n in xrange(16)])\n",
    "axes[1].legend(loc=4)\n",
    "for n in xrange(2):\n",
    "    axes[n].set_title(\"Growth of Error vs. $\\Delta x^n$\")\n",
    "    axes[n].set_xlabel(\"$\\Delta x$\")\n",
    "    axes[n].set_ylabel(\"Estimated Error\")\n",
    "    axes[n].set_title(\"Growth of different\")\n",
    "    axes[n].set_xlabel(\"$\\Delta x$\")\n",
    "    axes[n].set_ylabel(\"Estimated Error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Horner's Method for Evaluating Polynomials\n",
    "\n",
    "Given \n",
    "\n",
    "$$P_N(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_N x^N$$ \n",
    "\n",
    "or\n",
    "\n",
    "$$P_N(x) = p_1 x^N + p_2 x^{N-1} + p_3 x^{N-2} + \\ldots + p_{N+1}$$\n",
    "\n",
    "want to find best way to evaluate $P_N(x)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First consider two ways to write $P_3$:\n",
    "\n",
    "$$ P_3(x) = p_1 x^3 + p_2 x^2 + p_3 x + p_4$$\n",
    "\n",
    "and using nested multiplication:\n",
    "\n",
    "$$ P_3(x) = ((p_1 x + p_2) x + p_3) x + p_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider how many operations it takes for each...\n",
    "\n",
    "$$ P_3(x) = p_1 x^3 + p_2 x^2 + p_3 x + p_4$$\n",
    "\n",
    "$$P_3(x) = \\overbrace{p_1 \\cdot x \\cdot x \\cdot x}^3 + \\overbrace{p_2 \\cdot x \\cdot x}^2 + \\overbrace{p_3 \\cdot x}^1 + p_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding up all the operations we can in general think of this as a pyramid\n",
    "\n",
    "![Original Count](./images/horners_method_big_count.png)\n",
    "\n",
    "We can estimate this way that the algorithm written this way will take approximately $O(N^2 / 2)$ operations to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking at our other means of evaluation:\n",
    "\n",
    "$$ P_3(x) = ((p_1 x + p_2) x + p_3) x + p_4$$\n",
    "\n",
    "Here we find that the method is $O(N)$ (the 2 is usually ignored in these cases).  The important thing is that the first evaluation is $O(N^2)$ and the second $O(N)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "Fill in the function and implement Horner's method:\n",
    "```python\n",
    "def eval_poly(p, x):\n",
    "    \"\"\"Evaluates polynomial given coefficients p at x\n",
    "    \n",
    "    Function to evaluate a polynomial in order N operations.  The polynomial is defined as\n",
    "    \n",
    "    P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]\n",
    "    \n",
    "    The value x should be a float.\n",
    "    \"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def eval_poly(p, x):\n",
    "    \"\"\"Evaluates polynomial given coefficients p at x\n",
    "    \n",
    "    Function to evaluate a polynomial in order N operations.  The polynomial is defined as\n",
    "    \n",
    "    P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]\n",
    "    \n",
    "    The value x should be a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = p[0]\n",
    "    for coefficient in p[1:]:\n",
    "        y = y * x + coefficient\n",
    "    \n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "or an alternative version that allows `x` to be a vector of values:\n",
    "```python\n",
    "def eval_poly(p, x):\n",
    "    \"\"\"Evaluates polynomial given coefficients p at x\n",
    "    \n",
    "    Function to evaluate a polynomial in order N operations.  The polynomial is defined as\n",
    "    \n",
    "    P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]\n",
    "    \n",
    "    The value x can by a NumPy ndarray.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = numpy.ones(x.shape) * p[0]\n",
    "    for coefficient in p[1:]:\n",
    "        y = y * x + coefficient\n",
    "    \n",
    "    return y\n",
    "```\n",
    "This version calculates each `y` value simultaneously making for much faster code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def eval_poly(p, x):\n",
    "    \"\"\"Evaluates polynomial given coefficients p at x\n",
    "    \n",
    "    Function to evaluate a polynomial in order N operations.  The polynomial is defined as\n",
    "    \n",
    "    P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]\n",
    "    \n",
    "    The value x can by a NumPy ndarray.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = numpy.ones(x.shape) * p[0]\n",
    "    for coefficient in p[1:]:\n",
    "        y = y * x + coefficient\n",
    "    \n",
    "    return y\n",
    "\n",
    "p = [1, -3, 10, 4, 5, 5]\n",
    "x = numpy.linspace(-10, 10, 100)\n",
    "plt.plot(x, eval_poly(p, x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Truncation Error vs. Floating Point Error\n",
    "\n",
    "Truncation error:  Errors arising from approximation of a function, truncation of a series...\n",
    "\n",
    "$$\\sin x \\approx x - \\frac{x^3}{3!} + \\frac{x^5}{5!} + O(x^7)$$\n",
    "\n",
    "Floating-point Error:  Errors arising from approximating real numbers with finite-precision numbers\n",
    "\n",
    "$$\\pi \\approx 3.14$$\n",
    "\n",
    "or $\\frac{1}{3} \\approx 0.333333333$ in decimal, results form finitely number of registers to represent each number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating Point Systems\n",
    "\n",
    "Numbers in floating point systems are represented as a series of bits that represent different pieces of a number.  In *normalized floating point systems* there are some standar conventions for what these bits are used for.  In general the numbers are stored by breaking them down into the form  \n",
    "\n",
    "$$\\hat{f} = \\pm d_1 . d_2 d_3 d_4 \\ldots d_p \\times \\beta^E$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\hat{f} = \\pm d_1 . d_2 d_3 d_4 \\ldots d_p \\times \\beta^E$$\n",
    "\n",
    "where\n",
    "1. $\\pm$ is a single bit and of course represents the sign of the number\n",
    "2. $d_1 . d_2 d_3 d_4 \\ldots d_p$ is called the *mantissa*.  Note that technically the decimal could be moved but generally, using scientific notation, the decimal can always be placed at this location.  The digits $d_2 d_3 d_4 \\ldots d_p$ are called the *fraction* with $p$ digits of precision.  Normalized systems specifically put the decimal point in the front like we have and assume $d_1 \\neq 0$ unless the number is exactly $0$.\n",
    "3. $\\beta$ is the *base*.  For binary $\\beta = 2$, for decimal $\\beta = 10$, etc.\n",
    "4. $E$ is the *exponent*, an integer in the range $[E_{\\min}, E_{\\max}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The important points on any floating point system is that\n",
    "1. There exist a discrete and finite set of representable numbers\n",
    "2. These representable numbers are not evenly distributed on the real line\n",
    "3. Arithmetic in floating point systems yield different results from infinite precision arithmetic (i.e. \"real\" math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Floating Point Systems\n",
    "All floating-point systems are characterized by several important numbers\n",
    " - Smalled normalized number (underflow if below)\n",
    " - Largest normalized number (overflow if above)\n",
    " - Zero\n",
    " - Machine $\\epsilon$ or $\\epsilon_{\\text{machine}}$\n",
    " - `inf` and `nan`, infinity and **N**ot **a** **N**umber respectively\n",
    " - Subnormal numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example:  Toy System\n",
    "Consider the toy 2-digit precision decimal system (normalized)\n",
    "$$f = \\pm d_1 . d_2 \\times 10^E$$\n",
    "with $E \\in [-2, 0]$.\n",
    "\n",
    "#### Number and distribution of numbers\n",
    "1. How many numbers can we represent with this system?\n",
    "\n",
    "2. What is the distribution on the real line?\n",
    "\n",
    "3. What is the underflow and overflow limits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How many numbers can we represent with this system?\n",
    "\n",
    "$f = \\pm d_1 . d_2 \\times 10^E$ with $E \\in [-2, 0]$.\n",
    "\n",
    "$$ 2 \\times 9 \\times 10 \\times 3 + 1 = 541$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the distribution on the real line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "d_1_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "d_2_values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "E_values = [0, -1, -2]\n",
    "\n",
    "fig = plt.figure(figsize=(10.0, 1.0))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for E in E_values:\n",
    "    for d1 in d_1_values:\n",
    "        for d2 in d_2_values:\n",
    "            axes.plot( (d1 + d2 * 0.1) * 10**E, 0.0, 'r+', markersize=20)\n",
    "            axes.plot(-(d1 + d2 * 0.1) * 10**E, 0.0, 'r+', markersize=20)\n",
    "            \n",
    "axes.plot(0.0, 0.0, '+', markersize=20)\n",
    "axes.plot([-10.0, 10.0], [0.0, 0.0], 'k')\n",
    "\n",
    "axes.set_title(\"Distribution of Values\")\n",
    "axes.set_yticks([])\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"\")\n",
    "axes.set_xlim([-0.1, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the underflow and overflow limits?\n",
    "\n",
    "Smallest number that can be represented is the underflow:  $1.0 \\times 10^{-2} = 0.01$\n",
    "Largest number that can be represented is the overflow:  $9.9 \\times 10^0 = 9.9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Systems\n",
    "Consider the 2-digit precision base 2 system:\n",
    "\n",
    "$$f=\\pm d_1 . d_2 \\times 2^E ~~~~ \\text{with} ~~~~ E \\in [-1, 1]$$\n",
    "\n",
    "#### Number and distribution of numbers\n",
    "1. How many numbers can we represent with this system?\n",
    "\n",
    "2. What is the distribution on the real line?\n",
    "\n",
    "3. What is the underflow and overflow limits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How many numbers can we represent with this system?\n",
    "\n",
    "$$f=\\pm d_1 . d_2 \\times 2^E ~~~~ \\text{with} ~~~~ E \\in [-1, 1]$$\n",
    "\n",
    "$$ 2 \\times 1 \\times 2 \\times 3 + 1 = 13$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the distribution on the real line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "d_1_values = [1]\n",
    "d_2_values = [0, 1]\n",
    "E_values = [1, 0, -1]\n",
    "\n",
    "fig = plt.figure(figsize=(10.0, 1.0))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for E in E_values:\n",
    "    for d1 in d_1_values:\n",
    "        for d2 in d_2_values:\n",
    "            axes.plot( (d1 + d2 * 0.5) * 2**E, 0.0, 'r+', markersize=20)\n",
    "            axes.plot(-(d1 + d2 * 0.5) * 2**E, 0.0, 'r+', markersize=20)\n",
    "            \n",
    "axes.plot(0.0, 0.0, 'r+', markersize=20)\n",
    "axes.plot([-4.5, 4.5], [0.0, 0.0], 'k')\n",
    "\n",
    "axes.set_title(\"Distribution of Values\")\n",
    "axes.set_yticks([])\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"\")\n",
    "axes.set_xlim([-3.5, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Smallest number that can be represented is the underflow:  $1.0 \\times 2^{-1} = 0.5$\n",
    "Largest number that can be represented is the overflow:  $1.1 \\times 2^1 = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Systems - IEEE 754 Binary Floating Point Systems\n",
    "\n",
    "### Single Precision\n",
    " - Total storage alloted is 32 bits\n",
    " - Exponent is 8 bits $\\Rightarrow E \\in [-126, 127]$\n",
    " - Fraction 23 bits ($p = 24$)\n",
    " \n",
    "```\n",
    "s EEEEEEEE FFFFFFFFFFFFFFFFFFFFFFF\n",
    "0 1      8 9                     31\n",
    "```\n",
    "Overflow $= 2^{127} \\approx 3.4 \\times 10^{38}$\n",
    "\n",
    "Underflow $= 2^{-126} \\approx 1.2 \\times 10^{-38}$\n",
    "\n",
    "$\\epsilon_{\\text{machine}} = 2^{-23} \\approx 1.2 \\times 10^{-7}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Double Precision\n",
    " - Total storage alloted is 64 bits\n",
    " - Exponent is 11 bits $\\Rightarrow E \\in [-1022, 1024]$\n",
    " - Fraction 52 bits ($p = 53$)\n",
    " \n",
    "```\n",
    "s EEEEEEEEEE FFFFFFFFFF FFFFFFFFFF FFFFFFFFFF FFFFFFFFFF FFFFFFFFFF FF\n",
    "0 1       11 12                                                      63\n",
    "```\n",
    "Overflow $= 2^{1024} \\approx 1.8 \\times 10^{308}$\n",
    "\n",
    "Underflow $= 2^{-1022} \\approx 2.2 \\times 10^{-308}$\n",
    "\n",
    "$\\epsilon_{\\text{machine}} = 2^{-52} \\approx 2.2 \\times 10^{-16}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python Access to IEEE Numbers\n",
    "\n",
    "Access many important parameters, such as machine epsilon:\n",
    "\n",
    "```python\n",
    "import numpy\n",
    "numpy.finfo(float).eps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print numpy.finfo(float).eps\n",
    "print numpy.finfo(float).max\n",
    "print numpy.finfo(float).min\n",
    "print numpy.finfo(float).nmant\n",
    "print numpy.finfo(float).precision\n",
    "print numpy.finfo(float).tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should we care about this?\n",
    "\n",
    " - Floating point arithmetic is not commutative or associative\n",
    " - Floating point errors compound, do not assume even double precision is enough!\n",
    " - Mixing precision is very dangerous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### Examples\n",
    " \n",
    "Simple arithmetic $\\delta < \\epsilon_{\\text{machine}}$\n",
    "\n",
    "   $$(1+\\delta) - 1 = 1 - 1 = 0$$\n",
    "\n",
    "   $$1 - 1 + \\delta = \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Catastrophic cancelation (assume $x + y \\neq 0$):\n",
    "\n",
    "$$x(1 + \\epsilon_x) + y (1 + \\epsilon_y) = x + y + x \\epsilon_x + y \\epsilon_y =(x+y) \\left( 1 + \\frac{x \\epsilon_x + y \\epsilon_y}{x+y} \\right ) \\Rightarrow$$\n",
    "\n",
    "$$\\epsilon_{x+y} = \\frac{x}{x + y} \\epsilon_x  + \\frac{y}{x+y} \\epsilon_y$$\n",
    "\n",
    "This implies that the error made from doing the actual sum can be arbitrarily large!  This is not too bad unless $x$ and $y$ have opposite sign, then if $x+y ~ 0$ then $\\frac{1}{x+y}$ can be huge.  This is refered to as *catastrophic* cancelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix vector multiplication\n",
    "\n",
    "   $$ \\begin{bmatrix} \\epsilon & 1 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \n",
    "= \\begin{bmatrix} \\epsilon + 1 \\\\ 1 + 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%precision 16\n",
    "A = numpy.array([[1e-16, 1.0], [1.0, 1.0]])\n",
    "x = numpy.array([[1], [1]])\n",
    "numpy.dot(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluation of a polynomial\n",
    "\n",
    "   $$f(x) = x^7 - 7x^6 + 21 x^5 - 35 x^4 + 35x^3-21x^2 + 7x - 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(0.988, 1.012, 1000)\n",
    "y = x**7 - 7.0 * x**6 + 21.0 * x**5 - 35.0 * x**4 + 35.0 * x**3 - 21.0 * x**2 + 7.0 * x - 1.0\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x, y, 'r')\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute $f(x) = x + 1$ by the function $$\\hat{f}(x) = \\frac{x^2 - 1}{x - 1}$$.\n",
    "\n",
    "Do you expect there to be issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(0.5, 1.5, 101)\n",
    "f_hat = (x**2 - 1.0) / (x - 1.0)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x, numpy.abs(f_hat - (x + 1.0)))\n",
    "axes.set_xlabel(\"$x$\")\n",
    "axes.set_ylabel(\"Absolute Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more examples see the \"[100-digit Challenge](http://bookstore.siam.org/ot86/)\" book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combination of Error\n",
    "\n",
    "In general we need to concern ourselves with the combination of both truncation error and floating point error.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Consider the finite difference approximation where $f(x) = e^x$ and we are evaluating at $x=1$?\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$\n",
    "\n",
    "Compare the error between decreasing $\\Delta x$ and the true solution $f'(1) = e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "delta_x = numpy.linspace(1e-20, 5.0, 100)\n",
    "x = 1.0\n",
    "f_hat_1 = (numpy.exp(x + delta_x) - numpy.exp(x)) / (delta_x)\n",
    "f_hat_2 = (numpy.exp(x + delta_x) - numpy.exp(x - delta_x)) / (2.0 * delta_x)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.loglog(delta_x, numpy.abs(f_hat_1 - numpy.exp(1)), label=\"One-Sided\")\n",
    "axes.loglog(delta_x, numpy.abs(f_hat_2 - numpy.exp(1)), label=\"Centered\")\n",
    "axes.legend(loc=3)\n",
    "axes.set_xlabel(\"$\\Delta x$\")\n",
    "axes.set_ylabel(\"Absolute Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2\n",
    "\n",
    "Evaluate $e^x$ with its taylor series.\n",
    "\n",
    "$$e^x = \\sum^\\infty_{n=0} \\frac{x^n}{n!}$$\n",
    "\n",
    "Can we pick $N < \\infty$ that can approximate $e^x$ over a give range $x \\in [a,b]$ such that the relative error $E$ satisfies $E < 8 \\cdot \\varepsilon_{\\text{machine}}$?\n",
    "\n",
    "What might be a better way than simply evaluating the Taylor polynomial directly for various $N$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "def my_exp(x, N=10):\n",
    "    value = 0.0\n",
    "    for n in xrange(N + 1):\n",
    "        value += x**n / scipy.misc.factorial(n)\n",
    "        \n",
    "    return value\n",
    "\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "for N in range(1, 50):\n",
    "    error = numpy.abs((numpy.exp(x) - my_exp(x, N=N)) / numpy.exp(x))\n",
    "    if numpy.all(error < 8.0 * numpy.finfo(float).eps):\n",
    "        break\n",
    "\n",
    "print N\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x, error)\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"Relative Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some other links that might be helpful regarding IEEE Floating Point:\n",
    " - [What Every Computer Scientist Should Know About Floating-Point Arithmetic](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)\n",
    " - [IEEE 754 Floating Point Calculator](http://babbage.cs.qc.edu/courses/cs341/IEEE-754.html)\n",
    " - [Numerical Computing with IEEE Floating Point Arithmetic](http://epubs.siam.org/doi/book/10.1137/1.9780898718072)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
